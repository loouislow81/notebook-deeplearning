<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <link rel="stylesheet" href="assets/style.css?t=a44c9e9d">
    <script src="assets/script.js?t=820b7ffa"></script>
    <title>Neural Networks &amp; Deep Learning</title>
    <meta name="viewport" content="width=device-width">
  </head>
  <body class="-menu-visible">
    <div class="doc-layout">
      <div class="toggle menu-toggle js-menu-toggle"></div>
      <div class="body page-chapter-1">
        <div class="header-nav">
          <div class="right">
          </div>
        </div>
        <div class="markdown-body"><h1 id="neural-networks-and-deep-learning">Neural Networks &amp; Deep Learning</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#table-of-contents">Table of Contents</a></li>
<li><a href="#summary">Summary</a></li>
<li><a href="#introduction-to-deep-learning">Introduction to Deep Learning</a>
<ul>
<li><a href="#what-is-a-neural-network-nn">What is a Neural Network or NN?</a></li>
<li><a href="#supervised-learning-with-neural-networks">Supervised Learning with Neural Networks</a></li>
<li><a href="#why-is-deep-learning-taking-off">Why is Deep Learning taking off?</a></li>
</ul>
</li>
<li><a href="#neural-networks-basics">Neural Networks Basics</a>
<ul>
<li><a href="#binary-classification">Binary Classification</a></li>
<li><a href="#logistic-regression">Logistic Regression</a></li>
<li><a href="#logistic-regression-cost-function">Logistic Regression Cst Function</a></li>
<li><a href="#gradient-descent">Gradient Descent</a></li>
<li><a href="#derivatives">Derivatives</a></li>
<li><a href="#more-derivatives-examples">More Derivatives Examples</a></li>
<li><a href="#computation-graph">Computation Graph</a></li>
<li><a href="#derivatives-with-a-computation-graph">Derivatives with a Computation Graph</a></li>
<li><a href="#logistic-regression-gradient-descent">Logistic Regression Gradient Descent</a></li>
<li><a href="#gradient-descent-on-m-examples">Gradient Descent on m Examples</a></li>
<li><a href="#vectorization">Vectorization</a></li>
<li><a href="#vectorizing-logistic-regression">Vectorizing Logistic Regression</a></li>
<li><a href="#notes-on-python-and-numpy">Notes on Python and NumPy</a></li>
<li><a href="#general-notes">General Notes</a></li>
</ul>
</li>
<li><a href="#shallow-neural-networks">Shallow Neural Networks</a>
<ul>
<li><a href="#neural-networks-overview">Neural Networks Overview</a></li>
<li><a href="#neural-network-representation">Neural Network Representation</a></li>
<li><a href="#computing-a-neural-networks-output">Computing A Neural Network&apos;s Output</a></li>
<li><a href="#vectorizing-across-multiple-examples">Vectorizing Across Multiple Examples</a></li>
<li><a href="#activation-functions">Activation Functions</a></li>
<li><a href="#why-do-you-need-non-linear-activation-functions">Why Do You Need Non-Linear Activation Functions?</a></li>
<li><a href="#derivatives-of-activation-functions">Derivatives of Activation Functions</a></li>
<li><a href="#gradient-descent-for-neural-networks">Gradient Descent for Neural Networks</a></li>
<li><a href="#random-initialization">Random Initialization</a></li>
</ul>
</li>
<li><a href="#deep-neural-networks">Deep Neural Networks</a>
<ul>
<li><a href="#deep-l-layer-neural-network">Deep L-layer Neural Network</a></li>
<li><a href="#forward-propagation-in-a-deep-network">Forward Propagation in a Deep Network</a></li>
<li><a href="#getting-your-matrix-dimensions-right">Getting Your Matrix Dimensions Right</a></li>
<li><a href="#why-deep-representations">Why Deep Representations?</a></li>
<li><a href="#building-blocks-of-deep-neural-networks">Building Blocks of Deep Neural Networks</a></li>
<li><a href="#forward-and-backward-propagation">Forward and Backward Propagation</a></li>
<li><a href="#parameters-vs-hyperparameters">Parameters vs Hyperparameters</a></li>
<li><a href="#what-does-this-have-to-do-with-the-brain">What Does This Have To Do With The Brain</a></li>
</ul>
</li>
</ul>
<h2 id="summary">Summary</h2>
<blockquote>
<p>The foundations of deep learning:</p>
<ul>
<li>Understand the major technology trends driving Deep Learning.</li>
<li>Be able to build, train and apply fully connected deep neural networks.</li>
<li>Know how to implement efficient (vectorized) neural networks.</li>
<li>Understand the key parameters in a neural network&apos;s architecture.</li>
</ul>
</blockquote>
<h2 id="introduction-to-deep-learning">Introduction to Deep Learning</h2>
<blockquote>
<p>Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.</p>
</blockquote>
<h3 id="what-is-a-neural-network-or-nn">What is a Neural Network or NN?</h3>
<ul>
<li>Single neuron == linear regression</li>
<li>Simple NN graph:
<ul>
<li><img src="images/01/others/01.jpg" alt=""></li>
</ul>
</li>
<li>RELU stands for rectified linear unit is the most popular activation function right now that makes deep NNs train faster now.</li>
<li>Hidden layers predicts connection between inputs automatically, thats what deep learning is good at.</li>
<li>Deep NN consists of more hidden layers (Deeper layers).
<ul>
<li><img src="images/01/others/02.png" alt=""></li>
</ul>
</li>
<li>Each Input will be connected to the hidden layer and the NN will decide the connections.</li>
<li>Supervised learning means we have the (X,Y) and we need to get the function that maps X to Y.</li>
</ul>
<h3 id="supervised-learning-with-neural-networks">Supervised Learning with Neural Networks</h3>
<ul>
<li>Different types of neural networks for supervised learning which includes:
<ul>
<li>CNN or convolutional neural networks (Useful in computer vision).</li>
<li>RNN or Recurrent neural networks (Useful in Speech recognition or NLP).</li>
<li>Standard NN (Useful for Structured data).</li>
<li>Hybrid/custom NN or a Collection of NNs types.</li>
</ul>
</li>
<li>Structured data is like the databases and tables.</li>
<li>Unstructured data is like images, video, audio, and text.</li>
<li>Structured data gives more money because companies relies on prediction on its big data.</li>
</ul>
<h3 id="why-is-deep-learning-taking-off">Why is Deep Learning taking off?</h3>
<ul>
<li>Deep learning is taking off for 3 reasons:
<ol>
<li>Data:
<ul>
<li>Using this image we can conclude:
<img src="images/01/11.png" alt=""></li>
<li>For small data NN can perform as Linear regression or SVM (Support vector machine).</li>
<li>For big data a small NN is better that SVM.</li>
<li>For big data a big NN is better that a medium NN is better that small NN.</li>
<li>Hopefully we have a lot of data because the world is using the computer a little bit more:
<ul>
<li>Mobiles</li>
<li>IOT (Internet of things)</li>
</ul>
</li>
</ul>
</li>
<li>Computation:
<ul>
<li>GPUs</li>
<li>Powerful CPUs</li>
<li>Distributed Computing</li>
<li>ASICs</li>
</ul>
</li>
<li>Algorithm:
<ol>
<li>Creative algorithms has appeared that changed the way NN works.
<ul>
<li>For example using RELU function is so much better than using SIGMOID function in training a NN because it helps with the vanishing gradient problem.</li>
</ul>
</li>
</ol>
</li>
</ol>
</li>
</ul>
<h2 id="neural-networks-basics">Neural Networks Basics</h2>
<blockquote>
<p>Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.</p>
</blockquote>
<h3 id="binary-classification">Binary Classification</h3>
<ul>
<li>Mainly let&apos;s talk about how to do a logistic regression to make a binary classifier.
<img src="images/01/others/03.png" alt="log"></li>
<li>About an example of knowing if the current image contains a cat or not.</li>
<li>Here are some notations:
<ul>
<li><code>M is the number of training vectors</code></li>
<li><code>Nx is the size of the input vector</code></li>
<li><code>Ny is the size of the output vector</code></li>
<li><code>X(1) is the first input vector</code></li>
<li><code>Y(1) is the first output vector</code></li>
<li><code>X = [x(1) x(2).. x(M)]</code></li>
<li><code>Y = (y(1) y(2).. y(M))</code></li>
</ul>
</li>
<li>Use python as primary programming language.</li>
<li>In NumPy, make matrices and make operations on them in a fast and reliable time.</li>
</ul>
<h3 id="logistic-regression">Logistic Regression</h3>
<ul>
<li>Algorithm is used for classification algorithm of 2 classes.</li>
<li>Equations:
<ul>
<li>Simple equation:	<code>y = wx + b</code></li>
<li>If x is a vector: <code>y = w(transpose)x + b</code></li>
<li>If we need y to be in between 0 and 1 (probability): <code>y = sigmoid(w(transpose)x + b)</code></li>
<li>In some notations this might be used: <code>y = sigmoid(w(transpose)x)</code>
<ul>
<li>While <code>b</code> is <code>w0</code> of <code>w</code> and we add <code>x0 = 1</code>. but we won&apos;t use this notation in the example (because the first notation is better).</li>
</ul>
</li>
</ul>
</li>
<li>In binary classification <code>Y</code> has to be between <code>0</code> and <code>1</code>.</li>
<li>In the last equation <code>w</code> is a vector of <code>Nx</code> and <code>b</code> is a real number.</li>
</ul>
<h3 id="logistic-regression-cost-function">Logistic Regression Cost Function</h3>
<ul>
<li>First loss function would be the square root error:  <code>L(y&apos;,y) = 1/2 (y&apos; - y)^2</code>
<ul>
<li>But we won&apos;t use this notation because it leads us to optimization problem which is non convex, means it contains local optimum points.</li>
</ul>
</li>
<li>This is the function that we will use: <code>L(y&apos;,y) = - (y*log(y&apos;) + (1-y)*log(1-y&apos;))</code></li>
<li>To explain the last function lets see:
<ul>
<li>if <code>y = 1</code> ==&gt; <code>L(y&apos;,1) = -log(y&apos;)</code>  ==&gt; we want <code>y&apos;</code> to be the largest   ==&gt; <code>y</code>&apos; biggest value is 1</li>
<li>if <code>y = 0</code> ==&gt; <code>L(y&apos;,0) = -log(1-y&apos;)</code> ==&gt; we want <code>1-y&apos;</code> to be the largest ==&gt; <code>y&apos;</code> to be smaller as possible because it can only has 1 value.</li>
</ul>
</li>
<li>Then the Cost function will be: <code>J(w,b) = (1/m) * Sum(L(y&apos;[i],y[i]))</code></li>
<li>The loss function computes the error for a single training example; the cost function is the average of the loss functions of the entire training set.</li>
</ul>
<h3 id="gradient-descent">Gradient Descent</h3>
<ul>
<li>We want to predict <code>w</code> and <code>b</code> that minimize the cost function.</li>
<li>Our cost function is convex.</li>
<li>First we initialize <code>w</code> and <code>b</code> to 0,0 or initialize them to a random value in the convex function and then try to improve the values the reach minimum value.</li>
<li>In Logistic regression people always use 0,0 instead of random.</li>
<li>The gradient decent algorithm repeats: <code>w = w - alpha * dw</code>
where alpha is the learning rate and <code>dw</code> is the derivative of <code>w</code> (Change to <code>w</code>). The derivative is also the slope of <code>w</code>.</li>
<li>Looks like greedy algorithms. the derivative give us the direction to improve our parameters.</li>
<li>The actual equations we will implement:
<ul>
<li><code>w = w - alpha * d(J(w,b) / dw)</code>      (how much the function slopes in the w direction)</li>
<li><code>b = b - alpha * d(J(w,b) / db)</code>      (how much the function slopes in the d direction)</li>
</ul>
</li>
</ul>
<h3 id="derivatives">Derivatives</h3>
<ul>
<li>We will talk about some of required calculus.</li>
<li>You don&apos;t need to be a calculus geek to master deep learning but you&apos;ll need some skills from it.</li>
<li>Derivative of a linear line is its slope.
<ul>
<li>ex. <code>f(a) = 3a</code>     <code>d(f(a))/d(a) = 3</code></li>
<li>if <code>a = 2</code> then <code>f(a) = 6</code></li>
<li>if we move a a little bit <code>a = 2.001</code> then <code>f(a) = 6.003</code> means that we multiplied the derivative (Slope) to the moved area and added it to the last result.</li>
</ul>
</li>
</ul>
<h3 id="more-derivatives-examples">More Derivatives Examples</h3>
<ul>
<li><code>f(a) = a^2</code>  ==&gt; <code>d(f(a))/d(a) = 2a</code>
<ul>
<li><code>a = 2</code>  ==&gt; <code>f(a) = 4</code></li>
<li><code>a = 2.0001</code> ==&gt; <code>f(a) = 4.0004</code> approx.</li>
</ul>
</li>
<li><code>f(a) = a^3</code>  ==&gt; <code>d(f(a))/d(a) = 3a^2</code></li>
<li><code>f(a) = log(a)</code>  ==&gt; <code>d(f(a))/d(a) = 1/a</code></li>
<li>To conclude, Derivative is the slope and slope is different in different points in the function thats why the derivative is a function.</li>
</ul>
<h3 id="computation-graph">Computation Graph</h3>
<ul>
<li>Its a graph that organizes the computation from left to right.
<img src="images/01/02.png" alt=""></li>
</ul>
<h3 id="derivatives-with-a-computation-graph">Derivatives with a Computation Graph</h3>
<ul>
<li>Calculus chain rule says:
If <code>x -&gt; y -&gt; z</code>      (x effect y and y effects z)
Then <code>d(z)/d(x) = d(z)/d(y) * d(y)/d(x)</code></li>
<li>The video illustrates a big example.
<img src="images/01/03.png" alt=""></li>
<li>We compute the derivatives on a graph from right to left and it will be a lot more easier.</li>
<li><code>dvar</code> means the derivatives of a final output variable with respect to various intermediate quantities.</li>
</ul>
<h3 id="logistic-regression-gradient-descent">Logistic Regression Gradient Descent</h3>
<ul>
<li>In the video he discussed the derivatives of gradient decent example for one sample with two features <code>x1</code> and <code>x2</code>.
<img src="images/01/04.png" alt=""></li>
</ul>
<h3 id="gradient-descent-on-m-examples">Gradient Descent on <code>m</code> Examples</h3>
<ul>
<li>
<p>Lets say we have these variables:</p>
<pre><code>    X1      Feature
    X2      Feature
    W1      Weight of the first feature
    W2      Weight of the second feature
    B       Logistic Regression parameter
    M       Number of training examples
    Y(i)    Expected output of i
</code></pre>
</li>
<li>
<p>So we have:
<img src="images/01/09.png" alt=""></p>
</li>
<li>
<p>Then from right to left we will calculate derivations compared to the result:</p>
<pre><code>	d(a)  = d(l)/d(a) = -(y/a) + ((1-y)/(1-a))
	d(z)  = d(l)/d(z) = a - y
	d(W1) = X1 * d(z)
	d(W2) = X2 * d(z)
	d(B) = d(z)
</code></pre>
</li>
<li>
<p>From the above we can conclude the logistic regression pseudo code:</p>
<pre><code>	J = 0; dw1 = 0; dw2 =0; db = 0; # Devs
	w1 = 0; w2 = 0; b=0; # Weights

	for i = 1 to m

      # Forward pass
      z(i) = W1*x1(i) + W2*x2(i) + b
      a(i) = Sigmoid(z(i))
      J += (Y(i)*log(a(i)) + (1-Y(i))*log(1-a(i)))
    
      # Backward pass
      dz(i) = a(i) - Y(i)
      dw1 += dz(i) * x1(i)
      dw2 += dz(i) * x2(i)
      db  += dz(i)
    
      J /= m
      dw1/= m
      dw2/= m
      db/= m

  # Gradient descent
  w1 = w1 - alpa * dw1
  w2 = w2 - alpa * dw2
  b = b - alpa * db
</code></pre>
</li>
<li>
<p>The above code should run for some iterations to minimize error.</p>
</li>
<li>
<p>So there will be two inner loops to implement the logistic regression.</p>
</li>
<li>
<p>Vectorization is so important on deep learning to reduce loops. In the last code we can make the whole loop in one step using vectorization!</p>
</li>
</ul>
<h3 id="vectorization">Vectorization</h3>
<ul>
<li>Deep learning shines when the dataset are big. However for loops will make you wait a lot for a result. Thats why we need vectorization to get rid of some of our for loops.</li>
<li>NumPy library (dot) function is using vectorization by default.</li>
<li>The vectorization can be done on CPU or GPU thought the SIMD operation. But its faster on GPU.</li>
<li>Whenever possible avoid for loops.</li>
<li>Most of the NumPy library methods are vectorized version.</li>
</ul>
<h3 id="vectorizing-logistic-regression">Vectorizing Logistic Regression</h3>
<ul>
<li>
<p>We will implement Logistic Regression using one for loop then without any for loop.</p>
</li>
<li>
<p>As an input we have a matrix <code>X</code> and its <code>[Nx, m]</code> and a matrix <code>Y</code> and its <code>[Ny, m]</code>.</p>
</li>
<li>
<p>We will then compute at instance <code>[z1,z2...zm] = W&apos; * X + [b,b,...b]</code>. This can be written in python as:</p>
<pre><code>Z = np.dot(W.T,X) + b   # Vectorization Then broadcasting, Z shape is (1, m)
A = 1 / 1 + np.exp(-Z)  # Vectorization, A shape is (1, m)
</code></pre>
</li>
<li>
<p>Vectorizing Logistic Regression&apos;s Gradient Output:</p>
<pre><code>dz = A - Y    #Vectorization, dz shape is (1, m)
dw = np.dot(X,dz.T)/m   # Vectorization, dw shape is (Nx, 1)
db = dz.sum()/m   # Vectorization, dz shape is (1, 1)
</code></pre>
</li>
</ul>
<h3 id="notes-on-python-and-numpy">Notes on Python and NumPy</h3>
<ul>
<li>
<p>In NumPy, <code>obj.sum(axis = 0)</code> sums the columns while <code>obj.sum(axis = 1)</code> sums the rows.</p>
</li>
<li>
<p>In NumPy, <code>obj.reshape(1,4)</code> changes the shape of the matrix by broadcasting the values.</p>
</li>
<li>
<p>Reshape is cheap in calculations so put it everywhere you&apos;re not sure about the calculations.</p>
</li>
<li>
<p>Broadcasting works when you do a matrix operation with matrices that doesn&apos;t match for the operation, in this case NumPy automatically makes the shapes ready for the operation by broadcasting the values.</p>
</li>
<li>
<p>Some tricks to eliminate all the strange bugs in the code:</p>
<ul>
<li>If you didn&apos;t specify the shape of a vector, it will take a shape of <code>(m,)</code> and the transpose operation won&apos;t work. You have to reshape it to <code>(m, 1)</code>.</li>
<li>Try to not use the rank one matrix in ANN</li>
<li>Don&apos;t hesitate to use <code>assert(a.shape == (5,1))</code> to check if your matrix shape is the required one.</li>
<li>If you&apos;ve found a rank one matrix try to run reshape on it.</li>
</ul>
</li>
<li>
<p>Jupyter / IPython notebooks are so useful library in python that makes it easy to integrate code and document at the same time. It runs in the browser and doesn&apos;t need an IDE to run.</p>
<ul>
<li>To open Jupyter Notebook, open the command line and call: <code>jupyter-notebook</code> It should be installed to work.</li>
</ul>
</li>
<li>
<p>To Compute the derivative of Sigmoid:</p>
<pre><code>s = sigmoid(x)
ds = s * (1 - s)    # derivative  using calculus
</code></pre>
</li>
<li>
<p>To make an image of <code>(width,height,depth)</code> be a vector, use this:</p>
<pre><code>v = image.reshape(image.shape[0]*image.shape[1]*image.shape[2],1)  #reshapes the image.
</code></pre>
</li>
<li>
<p>Gradient descent converges faster after normalization of the input matrices.</p>
</li>
</ul>
<h3 id="general-notes">General Notes</h3>
<ul>
<li>The main steps for building a Neural Network are:
<ul>
<li>Define the model structure (such as number of input features and outputs)</li>
<li>Initialize the model&apos;s parameters</li>
<li>Loop:
<ul>
<li>Calculate current loss (forward propagation)</li>
<li>Calculate current gradient (backward propagation)</li>
<li>Update parameters (gradient descent)</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="shallow-neural-networks">Shallow Neural Networks</h2>
<blockquote>
<p>Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.</p>
</blockquote>
<h3 id="neural-networks-overview">Neural Networks Overview</h3>
<ul>
<li>
<p>In logistic regression we had:</p>
<pre><code>    X1  \  
    X2   ==&gt;  z = XW + B ==&gt; a = Sigmoid(z) ==&gt; l(a,Y)
    X3  /
</code></pre>
</li>
<li>
<p>In neural networks with one layer we will have:</p>
<pre><code>    X1  \  
    X2   =&gt;  z1 = XW1 + B1 =&gt; a1 = Sigmoid(a1) =&gt; z2 = a1W2 + B2 =&gt; a2 = Sigmoid(z2) =&gt; l(a2,Y)
    X3  /
</code></pre>
</li>
<li>
<p><code>X</code> is the input vector <code>(X1, X2, X3)</code>, and <code>Y</code> is the output variable <code>(1x1)</code>.</p>
</li>
<li>
<p>NN is stack of logistic regression objects.</p>
</li>
</ul>
<h3 id="neural-network-representation">Neural Network Representation</h3>
<ul>
<li>We will define the neural networks that has one hidden layer.</li>
<li>NN contains of input layers, hidden layers, output layers.</li>
<li>Hidden layer means we cant see that layers in the training set.</li>
<li><code>a0 = x</code> (the input layer).</li>
<li><code>a1</code> will represent the activation of the hidden neurons.</li>
<li><code>a2</code> will represent the output layer.</li>
<li>We are talking about 2 layers NN. The input layer isn&apos;t counted.</li>
</ul>
<h3 id="computing-a-neural-network&apos;s-output">Computing a Neural Network&apos;s Output</h3>
<ul>
<li>Equations of Hidden layers:
<img src="images/01/05.png" alt=""></li>
<li>Here are some informations about the last image:
<ul>
<li><code>noOfHiddenNeurons = 4</code></li>
<li><code>Nx = 3</code></li>
<li>Shapes of the variables:
<ul>
<li><code>W1</code> is the matrix of the first hidden layer, it has a shape of <code>(noOfHiddenNeurons,nx)</code></li>
<li><code>b1</code> is the matrix of the first hidden layer, it has a shape of <code>(noOfHiddenNeurons,1)</code></li>
<li><code>z1</code> is the result of the equation <code>z1 = W1*X + b</code>, it has a shape of <code>(noOfHiddenNeurons,1)</code></li>
<li><code>a1</code> is the result of the equation <code>a1 = sigmoid(z1)</code>, it has a shape of <code>(noOfHiddenNeurons,1)</code></li>
<li><code>W2</code> is the matrix of the second hidden layer, it has a shape of <code>(1,noOfHiddenLayers)</code></li>
<li><code>b2</code> is the matrix of the second hidden layer, it has a shape of <code>(1,1)</code></li>
<li><code>z2</code> is the result of the equation <code>z2 = W2*a1 + b</code>, it has a shape of <code>(1,1)</code></li>
<li><code>a2</code> is the result of the equation <code>a2 = sigmoid(z2)</code>, it has a shape of <code>(1,1)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="vectorizing-across-multiple-examples">Vectorizing Across Multiple Examples</h3>
<ul>
<li>
<p>Pseudo code for forward propagation for the 2 layers NN:</p>
<pre><code>for i = 1 to m
  	z[1, i] = W1*x[i] + b1        # shape of z[1, i] is (noOfHiddenNeurons,1)
  	a[1, i] = sigmoid(z[1, i])    # shape of a[1, i] is (noOfHiddenNeurons,1)
  	z[2, i] = W2*a[1, i] + b2     # shape of z[2, i] is (1,1)
  	a[2, i] = sigmoid(z[2, i])    # shape of a[2, i] is (1,1)
</code></pre>
</li>
<li>
<p>Lets say we have <code>X</code> on shape <code>(Nx ,m)</code>. So the new pseudo code:</p>
<pre><code>Z1 = W1X + b1       # shape of Z1 (noOfHiddenNeurons,m)
A1 = sigmoid(Z1)    # shape of A1 (noOfHiddenNeurons,m)
Z2 = W2A1 + b2      # shape of Z2 is (1,m)
A2 = sigmoid(Z2)    # shape of A2 is (1,m)
</code></pre>
</li>
<li>
<p>If you notice always m is the number of columns.</p>
</li>
<li>
<p>In the last example we can call <code>X</code>, <code>A0</code> for instance:</p>
<pre><code>Z1 = W1A0 + b1      # shape of Z1 (noOfHiddenNeurons,m)
A1 = sigmoid(Z1)    # shape of A1 (noOfHiddenNeurons,m)
Z2 = W2A1 + b2      # shape of Z2 is (1,m)
A2 = sigmoid(Z2)    # shape of A2 is (1,m)
</code></pre>
</li>
</ul>
<h3 id="activation-functions">Activation Functions</h3>
<ul>
<li>So far we are using sigmoid, but in some cases other functions can be a lot better.</li>
<li>Sigmoid can lead us to gradient decent problem where the updates are so low.</li>
<li>Sigmoid activation function range is [0,1]:
<code>A = 1 / (1 + np.exp(-z)) # Where z is the input matrix</code></li>
<li>Tanh activation function range is [-1,1] (Shifted version of sigmoid function).
<ul>
<li>
<p>In NumPy we can implement Tanh using one of these methods:
<code>A = (np.exp(z) - np.exp(-z)) / (np.exp(z) + np.exp(-z)) # Where z is the input matrix</code></p>
<p><strong>Or</strong></p>
<p><code>A = np.tanh(z) # Where z is the input matrix</code></p>
</li>
</ul>
</li>
<li>It turns out that using the Tanh function in hidden layers is far more better (Because of the zero mean of the function).</li>
<li>Sigmoid or Tanh function disadvantage is that if the input is too small or too high, the slope will be near zero which will cause us the gradient decent problem.</li>
<li>One of the popular activation functions that solved the slow gradient decent is the RELU function.
<code>RELU = max(0,z) # so if z is negative the slope is 0 and if z is positive the slope remains linear.</code></li>
<li>So here is some basic rule for choosing activation functions, if your classification is between 0 and 1, use the output activation as sigmoid and the others as RELU.</li>
<li>Leaky RELU activation function different of RELU is that if the input is negative the slope will be so small. It works as RELU but most people uses RELU.
<code>Leaky_RELU = max(0.01z,z) #the 0.01 can be a parameter for your algorithm.</code></li>
<li>In NN you will decide a lot of choices like:
<ul>
<li>No of hidden layers.</li>
<li>No of neurons in each hidden layer.</li>
<li>Learning rate. (The most important parameter)</li>
<li>Activation functions.</li>
<li>And others...</li>
</ul>
</li>
<li>It turns out there are no guide lines for that. You should try all activation functions for example.</li>
</ul>
<h3 id="why-do-you-need-non-linear-activation-functions">Why Do You Need Non-Linear Activation Functions?</h3>
<ul>
<li>If we removed the activation function from our algorithm that can be called linear activation function.</li>
<li>Linear activation function will output linear activations
<ul>
<li>Whatever hidden layers you add, the activation will be always linear like logistic regression (So its useless in a lot of complex problems).</li>
</ul>
</li>
<li>You might use this in one place, If the output is real numbers, you can use linear activation function in the output layer.</li>
</ul>
<h3 id="derivatives-of-activation-functions">Derivatives of Activation Functions</h3>
<ul>
<li>
<p>Derivation of Sigmoid activation function:</p>
<pre><code>g(z) = 1 / (1 + np.exp(-z))
g&apos;(z) = (1 / (1 + np.exp(-z))) * (1 - (1 / (1 + np.exp(-z))))
g&apos;(z) = g(z) * (1 - g(z))
</code></pre>
</li>
<li>
<p>Derivation of Tanh activation function:</p>
<pre><code>g(z)  = (e^z - e^-z) / (e^z + e^-z)
g&apos;(z) = 1 - np.tanh(z)^2 = 1 - g(z)^2
</code></pre>
</li>
<li>
<p>Derivation of RELU activation function:</p>
<pre><code>g(z)  = np.maximum(0,z)
g&apos;(z) = { 0  if z&lt;0
		  1  if z&gt;=0  }
</code></pre>
</li>
<li>
<p>Derivation of leaky RELU activation function:</p>
<pre><code>g(z)  = np.maximum(0.01 * z, z)
g&apos;(z) = { 0.01  if z&lt;0
				  1     if z&gt;=0 }
</code></pre>
</li>
</ul>
<h3 id="gradient-descent-for-neural-networks">Gradient Descent for Neural Networks</h3>
<ul>
<li>
<p>In this section we will have the full back propagation of the neural network (Just the equations with no explanations).</p>
</li>
<li>
<p>Gradient descent algorithm:</p>
<ul>
<li>
<p>NN parameters:</p>
<ul>
<li><code>n[0] = Nx</code></li>
<li><code>n[1] = NoOfHiddenNeurons</code></li>
<li><code>n[2] = NoOfOutputNeurons = 1</code></li>
<li><code>W1</code> shape is <code>(n[1],n[0])</code></li>
<li><code>b1</code> shape is <code>(n[1],1)</code></li>
<li><code>W2</code> shape is <code>(n[2],n[1])</code></li>
<li><code>b2</code> shape is <code>(n[2],1)</code></li>
</ul>
</li>
<li>
<p>Cost function <code>I = I(W1, b1, W2, b2) = (1/m) * Sum(L(Y,A2))</code></p>
</li>
<li>
<p>Then Gradient descent:</p>
<pre><code>Repeat:
  Compute predictions (y&apos;[i], i = 0,...m)
  Get derivatives: dW1, db1, dW2, db2
  Update: W1 = W1 - LearningRate * dW1
      b1 = b1 - LearningRate * db1
      W2 = W2 - LearningRate * dW2
      b2 = b2 - LearningRate * db2
</code></pre>
</li>
</ul>
</li>
<li>
<p>Forward propagation:</p>
<pre><code>Z1 = W1A0 + b1    # A0 is X
A1 = g1(Z1)
Z2 = W2A1 + b2
A2 = Sigmoid(Z2)  # Sigmoid because the output is between 0 and 1
</code></pre>
</li>
<li>
<p>Back propagation (The new thing / derivations):</p>
<pre><code>dZ2 = A2 - Y  # derivative of cost function we used * derivative of the sigmoid function
dW2 = (dZ2 * A1.T) / m
db2 = Sum(dZ2) / m
dZ1 = (W2.T * dZ2) * g&apos;1(Z1)    # element wise product (*)
dW2 = (dZ1 * A0.T) / m          # A0 = X
db2 = Sum(dZ1) / m
# Hint there are transposes when you are trying to multiplicate because these are matrices.
</code></pre>
</li>
<li>
<p>How we derived the 6 equations of the back propagation:
<img src="images/01/06.png" alt=""></p>
</li>
</ul>
<h3 id="random-initialization">Random Initialization</h3>
<ul>
<li>
<p>In logistic regression it wasn&apos;t important to initialize the weights randomly, while in NN we have to initialize them randomly.</p>
</li>
<li>
<p>If we initialize the weights with zeros in NN it won&apos;t work lets see why.</p>
</li>
<li>
<p>If we initialize <code>W</code> with zero, Then <code>A1[:,1]</code> will equal to <code>A[:,2]</code>. So <code>Z[:,1]</code> will equal <code>Z[:,2]</code> (We are talking in the middle layer).</p>
</li>
<li>
<p>Then all the hidden units will always updates the same.</p>
</li>
<li>
<p>To solve this we initialize the W&apos;s with a small random numbers:</p>
<pre><code>W1 = np.random.randn((2,2)) * 0.01    #0.01 to make it small enough
b1 = np.zeros((2,1))    # its ok to have b as zero, it won&apos;t get us to the symmetry problem.
</code></pre>
</li>
<li>
<p>We need small values because in sigmoid for example, if the number is big it will be 0 or 1 we will have flat parts. So learning will be so slow.</p>
</li>
<li>
<p>0.01 is alright for 1 hidden neurons, but if the NN is deep this number can be changed but it will always be a small number.</p>
</li>
</ul>
<h2 id="deep-neural-networks">Deep Neural Networks</h2>
<blockquote>
<p>Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.</p>
</blockquote>
<h3 id="deep-l-layer-neural-network">Deep L-layer Neural Network</h3>
<ul>
<li>Shallow NN is a NN with one or two layers.</li>
<li>Deep NN is a NN with three or more layers.</li>
<li>We will use the notation <code>L</code> to denote the number of layers in a NN.</li>
<li><code>n[l]</code> is the number of neurons in a specific layer <code>l</code>.</li>
<li><code>n[0]</code> denotes the number of neurons input layer. <code>n[L]</code> denotes the number of neurons in output layer.</li>
<li><code>g[l]</code> is the activation function.</li>
<li><code>a[l] = g[l](z[l])</code></li>
<li><code>w[l]</code> weights is used for <code>z[l]</code></li>
<li><code>x = a[0]</code>, <code>a[l] = y&apos;</code></li>
<li>These were the notation we will use for deep neural network.</li>
<li>So we have:
<ul>
<li>A vector <code>n</code> of shape <code>(1, NoOfLayers+1)</code></li>
<li>A vector <code>g</code> of shape <code>(1, NoOfLayers)</code></li>
<li>A list of different shapes <code>w</code> based on the number of neurons on the previous and the current layer.</li>
<li>A list of different shapes <code>b</code> based on the number of neurons on the current layer.</li>
</ul>
</li>
</ul>
<h3 id="forward-propagation-in-a-deep-network">Forward Propagation in a Deep Network</h3>
<ul>
<li>
<p>Forward propagation General rule for one input:</p>
<pre><code>z[l] = W[l]a[l-1] + b[l]
a[l] = g[l](a[l])
</code></pre>
</li>
<li>
<p>Forward propagation General rule for <code>m</code> inputs:</p>
<pre><code>Z[l] = W[l]A[l-1] + B[l]
A[l] = g[l](A[l])
</code></pre>
</li>
<li>
<p>We can&apos;t compute the whole layers forward propagation without a for loop so its OK to have a for loop here.</p>
</li>
<li>
<p>The dimensions of the matrices are so important you need to figure it out.</p>
</li>
</ul>
<h3 id="getting-your-matrix-dimensions-right">Getting your matrix dimensions right</h3>
<ul>
<li>The best way to debug your matrices dimensions is by a pencil and paper.</li>
<li>Dimension of <code>W</code> is <code>(n[l],n[l-1])</code> . Can be thought by Right to left.</li>
<li>Dimension of <code>b</code> is <code>(n[l],1)</code>.</li>
<li><code>dw</code> has the same shape as <code>W</code>, while <code>db</code> is the same shape as <code>b</code>.</li>
<li>Dimension of <code>Z[l],</code> <code>A[l]</code>, <code>dZ[l]</code>, and <code>dA[l]</code>  is <code>(n[l],m)</code>.</li>
</ul>
<h3 id="why-deep-representations">Why Deep Representations?</h3>
<ul>
<li>Why deep NN works well, we will discuss this question in this section.</li>
<li>Deep NN makes relations with data from simpler to complex. In each layer it tries to make a relations between the previous layer.</li>
<li>Face recognition application:
<ul>
<li>Image ==&gt; Edges ==&gt; Face parts ==&gt; Faces ==&gt; desired face</li>
</ul>
</li>
<li>Audio recognition application:
<ul>
<li>Audio ==&gt; Low level sound features like (sss,bb) ==&gt; Phonemes ==&gt; Words ==&gt; Sentences</li>
</ul>
</li>
<li>Neural Researchers thinks that deep neural networks thinks like brains (Simple ==&gt; Complex)</li>
<li>Circuit theory and deep learning:
<img src="images/01/07.png" alt=""></li>
<li>When starting on an application don&apos;t start directly by dozens of hidden layers. Try the simplest solutions (L Regression) then try the parameters then try the shallow neural network and so on.</li>
</ul>
<h3 id="building-blocks-of-deep-neural-networks">Building Blocks of Deep Neural Networks</h3>
<ul>
<li>Forward and back propagation for a layer l:
<img src="images/01/10.png" alt="Untitled"></li>
<li>Deep NN blocks:
<img src="images/01/08.png" alt=""></li>
</ul>
<h3 id="forward-and-backward-propagation">Forward and Backward Propagation</h3>
<ul>
<li>
<p>Pseudo code for forward propagation for layer l:</p>
<pre><code>Input  A[l-1]
Z[l] = W[l]A[l-1] + b[l]
A[l] = g[l](Z[l])
Output A[l], cache(Z[l])
</code></pre>
</li>
<li>
<p>Pseudo code for back propagation for layer l:</p>
<pre><code>Input da[l], Caches
dZ[l] = dA[l] * g&apos;[l](Z[l])
dW[l] = (dZ[l]A[l-1].T) / m
db[l] = sum(dZ[l])/m    # Dont forget axis=1, keepdims=True
dA[l-1] = w[l].T * dZ[1]    # The multiplication here are a dot product.
Output dA[l-1], dW[l], db[l]
</code></pre>
</li>
<li>
<p>If we have used our loss function then:</p>
<pre><code>dA[L] = (-(y/a) + ((1-y)/(1-a)))
</code></pre>
</li>
</ul>
<h3 id="parameters-vs-hyperparameters">Parameters vs Hyperparameters</h3>
<ul>
<li>Main parameters of the NN is <code>W</code> and <code>b</code></li>
<li>Hyper parameters (parameters that control the algorithm) are like:
<ul>
<li>Learning rate.</li>
<li>Number of iteration.</li>
<li>Number of hidden layers <code>L</code>.</li>
<li>Number of hidden units <code>n</code>.</li>
<li>Choice of activation functions.</li>
</ul>
</li>
<li>You have to try values yourself of hyper parameters.</li>
<li>In the old days they thought that learning rate is a parameter while now all knows its a hyper parameter.</li>
</ul>
<h3 id="what-does-this-have-to-do-with-the-brain">What Does This Have To Do With The Brain</h3>
<ul>
<li>No Human today understand how a human brain neuron works.</li>
<li>No Human today know exactly how many neurons on the brain.</li>
<li>NN is a small representation of how brain work. The most near model of human brain is in the computer vision (CNN).</li>
</ul>

        </div>
        <div class="footer-nav">
          <div class="left"><a href="index.html"><span class="title">Home</span></a></div>
          <div class="right"><a href="chapter-2.html"><span class="label">Next: </span><span class="title">Improving Deep Neural Networks</span></a></div>
        </div>
      </div>
      <div class="menu toc-menu">
        <li class="menu-item -level-0 -parent">
          <ul class="submenu">
            <li class="menu-item -level-1"><a href="index.html" class="link title  link-index">Home</a>
            </li>
            <li class="menu-item -level-1 -parent"><span class="title">Chapter 1</span>
              <ul class="submenu">
                <li class="menu-item -level-2"><a href="chapter-1.html" class="link title -active link-chapter-1">Neural Networks &amp; Deep Learning</a>
                  <ul class="headings heading-list">
                    <li class="heading-item -depth-2"><a href="#table-of-contents" class="hlink link-table-of-contents">Table of Contents</a>
                    </li>
                    <li class="heading-item -depth-2"><a href="#summary" class="hlink link-summary">Summary</a>
                    </li>
                    <li class="heading-item -depth-2"><a href="#introduction-to-deep-learning" class="hlink link-introduction-to-deep-learning">Introduction to Deep Learning</a>
                      <ul class="heading-list -depth-2">
                        <li class="heading-item -depth-3"><a href="#what-is-a-neural-network-or-nn" class="hlink link-what-is-a-neural-network-or-nn">What is a Neural Network or NN?</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#supervised-learning-with-neural-networks" class="hlink link-supervised-learning-with-neural-networks">Supervised Learning with Neural Networks</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#why-is-deep-learning-taking-off" class="hlink link-why-is-deep-learning-taking-off">Why is Deep Learning taking off?</a>
                        </li>
                      </ul>
                    </li>
                    <li class="heading-item -depth-2"><a href="#neural-networks-basics" class="hlink link-neural-networks-basics">Neural Networks Basics</a>
                      <ul class="heading-list -depth-2">
                        <li class="heading-item -depth-3"><a href="#binary-classification" class="hlink link-binary-classification">Binary Classification</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#logistic-regression" class="hlink link-logistic-regression">Logistic Regression</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#logistic-regression-cost-function" class="hlink link-logistic-regression-cost-function">Logistic Regression Cost Function</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#gradient-descent" class="hlink link-gradient-descent">Gradient Descent</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#derivatives" class="hlink link-derivatives">Derivatives</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#more-derivatives-examples" class="hlink link-more-derivatives-examples">More Derivatives Examples</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#computation-graph" class="hlink link-computation-graph">Computation Graph</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#derivatives-with-a-computation-graph" class="hlink link-derivatives-with-a-computation-graph">Derivatives with a Computation Graph</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#logistic-regression-gradient-descent" class="hlink link-logistic-regression-gradient-descent">Logistic Regression Gradient Descent</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#gradient-descent-on-m-examples" class="hlink link-gradient-descent-on-m-examples">Gradient Descent on `m` Examples</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#vectorization" class="hlink link-vectorization">Vectorization</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#vectorizing-logistic-regression" class="hlink link-vectorizing-logistic-regression">Vectorizing Logistic Regression</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#notes-on-python-and-numpy" class="hlink link-notes-on-python-and-numpy">Notes on Python and NumPy</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#general-notes" class="hlink link-general-notes">General Notes</a>
                        </li>
                      </ul>
                    </li>
                    <li class="heading-item -depth-2"><a href="#shallow-neural-networks" class="hlink link-shallow-neural-networks">Shallow Neural Networks</a>
                      <ul class="heading-list -depth-2">
                        <li class="heading-item -depth-3"><a href="#neural-networks-overview" class="hlink link-neural-networks-overview">Neural Networks Overview</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#neural-network-representation" class="hlink link-neural-network-representation">Neural Network Representation</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#computing-a-neural-network's-output" class="hlink link-computing-a-neural-network's-output">Computing a Neural Network's Output</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#vectorizing-across-multiple-examples" class="hlink link-vectorizing-across-multiple-examples">Vectorizing Across Multiple Examples</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#activation-functions" class="hlink link-activation-functions">Activation Functions</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#why-do-you-need-non-linear-activation-functions" class="hlink link-why-do-you-need-non-linear-activation-functions">Why Do You Need Non-Linear Activation Functions?</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#derivatives-of-activation-functions" class="hlink link-derivatives-of-activation-functions">Derivatives of Activation Functions</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#gradient-descent-for-neural-networks" class="hlink link-gradient-descent-for-neural-networks">Gradient Descent for Neural Networks</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#random-initialization" class="hlink link-random-initialization">Random Initialization</a>
                        </li>
                      </ul>
                    </li>
                    <li class="heading-item -depth-2"><a href="#deep-neural-networks" class="hlink link-deep-neural-networks">Deep Neural Networks</a>
                      <ul class="heading-list -depth-2">
                        <li class="heading-item -depth-3"><a href="#deep-l-layer-neural-network" class="hlink link-deep-l-layer-neural-network">Deep L-layer Neural Network</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#forward-propagation-in-a-deep-network" class="hlink link-forward-propagation-in-a-deep-network">Forward Propagation in a Deep Network</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#getting-your-matrix-dimensions-right" class="hlink link-getting-your-matrix-dimensions-right">Getting your matrix dimensions right</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#why-deep-representations" class="hlink link-why-deep-representations">Why Deep Representations?</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#building-blocks-of-deep-neural-networks" class="hlink link-building-blocks-of-deep-neural-networks">Building Blocks of Deep Neural Networks</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#forward-and-backward-propagation" class="hlink link-forward-and-backward-propagation">Forward and Backward Propagation</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#parameters-vs-hyperparameters" class="hlink link-parameters-vs-hyperparameters">Parameters vs Hyperparameters</a>
                        </li>
                        <li class="heading-item -depth-3"><a href="#what-does-this-have-to-do-with-the-brain" class="hlink link-what-does-this-have-to-do-with-the-brain">What Does This Have To Do With The Brain</a>
                        </li>
                      </ul>
                    </li>
                  </ul>
                </li>
              </ul>
            </li>
            <li class="menu-item -level-1 -parent"><span class="title">Chapter 2</span>
              <ul class="submenu">
                <li class="menu-item -level-2"><a href="chapter-2.html" class="link title  link-chapter-2">Improving Deep Neural Networks</a>
                </li>
              </ul>
            </li>
            <li class="menu-item -level-1 -parent"><span class="title">Chapter 3</span>
              <ul class="submenu">
                <li class="menu-item -level-2"><a href="chapter-3.html" class="link title  link-chapter-3">Structuring Machine Learning Projects</a>
                </li>
              </ul>
            </li>
            <li class="menu-item -level-1 -parent"><span class="title">Chapter 4</span>
              <ul class="submenu">
                <li class="menu-item -level-2"><a href="chapter-4.html" class="link title  link-chapter-4">Convolutional Neural Networks</a>
                </li>
              </ul>
            </li>
            <li class="menu-item -level-1 -parent"><span class="title">Chapter 5</span>
              <ul class="submenu">
                <li class="menu-item -level-2"><a href="chapter-5.html" class="link title  link-chapter-5">Sequence Models</a>
                </li>
              </ul>
            </li>
          </ul>
        </li>
      </div>
    </div>
  </body>
</html>